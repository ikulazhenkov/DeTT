{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN model with and without instrument inclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time, gc\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dl_model_utils import get_transform_train,get_transform_valid, Cholec80Dataset, EarlyStopping, start_timer, end_timer_and_print\n",
    "\n",
    "from src.models import ResNet18Model,ResNet18LSTM, ResNetBlock, ResNet50,ResNet50_ST,ResNet50_ST_Phase, ResNet50LSTM, Xception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUMENT_MAP = {\n",
    "    '':0,\n",
    "    'tool_Grasper':1,\n",
    "    'tool_Grasper, tool_Hook':2,\n",
    "    'tool_Hook':3,\n",
    "    'tool_Grasper, tool_Irrigator':4,\n",
    "    'tool_Irrigator':5,\n",
    "    'tool_Bipolar':6,\n",
    "    'tool_Grasper, tool_Bipolar':7,\n",
    "    'tool_Grasper, tool_Clipper':8,\n",
    "    'tool_Clipper':9,\n",
    "    'tool_Grasper, tool_Scissors':10,\n",
    "    'tool_SpecimenBag':11,\n",
    "    'tool_Grasper, tool_SpecimenBag':12,\n",
    "    'tool_Scissors':13,\n",
    "    'tool_Grasper, tool_Bipolar, tool_Irrigator':14,\n",
    "    'tool_Bipolar, tool_Irrigator':15,\n",
    "    'tool_Bipolar, tool_SpecimenBag':16,\n",
    "    'tool_Grasper, tool_Bipolar, tool_SpecimenBag':17,\n",
    "    'tool_Irrigator, tool_SpecimenBag':18,\n",
    "    'tool_Bipolar, tool_Irrigator, tool_SpecimenBag':19,\n",
    "    'tool_Grasper, tool_Irrigator, tool_SpecimenBag':20,\n",
    "    'tool_Grasper, tool_Clipper, tool_Irrigator':21,\n",
    "    'tool_Clipper, tool_Irrigator':22,\n",
    "    'tool_Grasper, tool_Clipper, tool_SpecimenBag':23,\n",
    "    'tool_Grasper, tool_Scissors, tool_SpecimenBag':24,\n",
    "    'tool_Hook, tool_Irrigator':25,\n",
    "    'tool_Grasper, tool_Hook, tool_Irrigator':26,\n",
    "    'tool_Bipolar, tool_Scissors':27\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Type : cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device Type : {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train and validation sets\n",
    "train_df = pd.read_parquet('data/ordered_train_df.parquet')\n",
    "val_df = pd.read_parquet('data/ordered_val_df.parquet')\n",
    "\n",
    "#Sample train and val datasets at 1fps\n",
    "train_df = train_df[train_df['frame'] % 25 == 0].reset_index(drop=True)\n",
    "val_df = val_df[val_df['frame'] % 25 == 0].reset_index(drop=True)\n",
    "\n",
    "train_df['tool_target'] = train_df[['tool_Grasper', 'tool_Bipolar',\n",
    "        'tool_Hook', 'tool_Scissors', 'tool_Clipper', 'tool_Irrigator',\n",
    "        'tool_SpecimenBag']].dot(train_df[['tool_Grasper', 'tool_Bipolar',\n",
    "        'tool_Hook', 'tool_Scissors', 'tool_Clipper', 'tool_Irrigator',\n",
    "        'tool_SpecimenBag']].columns + ', ').str.rstrip(', ').map(INSTRUMENT_MAP).values\n",
    "val_df['tool_target'] = val_df[['tool_Grasper', 'tool_Bipolar',\n",
    "        'tool_Hook', 'tool_Scissors', 'tool_Clipper', 'tool_Irrigator',\n",
    "        'tool_SpecimenBag']].dot(val_df[['tool_Grasper', 'tool_Bipolar',\n",
    "        'tool_Hook', 'tool_Scissors', 'tool_Clipper', 'tool_Irrigator',\n",
    "        'tool_SpecimenBag']].columns + ', ').str.rstrip(', ').map(INSTRUMENT_MAP).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_validate_cnn(train_df, val_df,epochs=100, model_num=0, optimizer_num = 0,batch_size = 128, learning_rate = 1e-3, weight_decay=5e-4,momentum=0.9,patience=20, patience_delta = 0.0005):\n",
    "    '''\n",
    "    Model nums:\n",
    "\n",
    "    Model 0: ResNet18\n",
    "    Model 1: Resnet50 - Pre-Trained\n",
    "    Model 2: ResNet18 w/LSTM\n",
    "    Model 3: ResNet50 - Pre-Trained w/LSTM\n",
    "    Model 4: Xception - Pre-Trained\n",
    "    Model 5: Resnet50 - OD Phase\n",
    "\n",
    "    '''\n",
    "\n",
    "    #Get train label weights and convert to torch tensor\n",
    "    phase_weights=class_weight.compute_class_weight('balanced',classes=np.unique(train_df['phase']),y=train_df['phase'].to_numpy())\n",
    "    phase_weights=torch.tensor(phase_weights,dtype=torch.float)\n",
    "\n",
    "    #Create datasets for phase segmentation\n",
    "    trainset = Cholec80Dataset(train_df,  get_transform_train())\n",
    "    validationset = Cholec80Dataset(val_df,  get_transform_valid())\n",
    "\n",
    "    #Shuffle for non-lstm model. For lstm we preserve the sequence\n",
    "    if model_num == 2 or model_num == 3:\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "\n",
    "    validationloader = torch.utils.data.DataLoader(\n",
    "        validationset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    #Initialise two required loss functions\n",
    "    criterion = nn.CrossEntropyLoss(weight=phase_weights)\n",
    "    criterion_tool = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    if device == 'cuda':\n",
    "        criterion.to(device)\n",
    "  \n",
    "\n",
    "    #Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience, patience_delta)\n",
    "\n",
    "\n",
    "    #Model selection\n",
    "    if model_num == 0:\n",
    "        model = ResNet18Model(ResNetBlock)\n",
    "        model_name = 'resnet_18_ord'\n",
    "        \n",
    "    elif model_num == 1:\n",
    "        model = ResNet50()\n",
    "        model.freeze()\n",
    "        model_name = 'resnet_50_ord'\n",
    "    elif model_num == 2:\n",
    "        model = ResNet18LSTM()\n",
    "        model_name = 'resnet_18_lstm_ord'\n",
    "    elif model_num == 3:\n",
    "        model = ResNet50LSTM()\n",
    "        model_name = 'resnet_50_lstm_ord'\n",
    "    elif model_num == 4:\n",
    "        model = Xception()\n",
    "        model.freeze()\n",
    "        model_name = 'xception_ord'\n",
    "    elif model_num == 5:\n",
    "        model = ResNet50_ST_Phase()\n",
    "        model.freeze()\n",
    "        model_name = 'resnet_50_od_phase_ord'\n",
    "\n",
    "\n",
    "    #Select optimizer for job\n",
    "    optimizer = None\n",
    "    if optimizer_num == 0:\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=momentum,weight_decay=weight_decay)\n",
    "    elif optimizer_num == 1:\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        model.to(device)\n",
    "\n",
    "    start_timer()\n",
    "    for epoch in range(0,epochs):\n",
    "        print(f'Training Epoch: {epoch+1} of {epochs}..')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0.0\n",
    "\n",
    "        # Outputs for HMM\n",
    "        img_path = []\n",
    "        true_labels = []\n",
    "        soft_max_out = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        model.train()\n",
    "        for i, (train_inputs,train_img_id, train_labels,train_tools,train_tool_targets) in tqdm(enumerate(trainloader, 0)):\n",
    "            img_path = img_path + list(train_img_id)\n",
    "            true_labels = true_labels + train_labels.tolist()\n",
    "            if device == 'cuda':\n",
    "                train_inputs, train_labels, train_tools = train_inputs.to(device), train_labels.to(device), train_tools.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # if model_num < 2:\n",
    "            if model_num != 5:\n",
    "                train_outputs  = model.forward(train_inputs)\n",
    "            else:\n",
    "                train_outputs, _ = model.forward(train_inputs)\n",
    "            \n",
    "            _, train_preds = torch.max(train_outputs.data, 1)\n",
    "            loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "                \n",
    "            # else:\n",
    "\n",
    "                  \n",
    "                \n",
    "                \n",
    "            #     tool_outputs, train_outputs  = model.forward(train_inputs)\n",
    "            #     tool_outputs = tool_outputs.data\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "            #     train_tool_outputs = (tool_outputs > 0.5).float()\n",
    "\n",
    "                \n",
    "            #     train_tool_outputs = train_tool_outputs.float()\n",
    "\n",
    "            #     tool_l = criterion_tool(train_tool_outputs, train_tools.float())\n",
    "\n",
    "            #     _, train_preds = torch.max(train_outputs.data, 1)\n",
    "\n",
    "            #     phase_l = criterion(train_outputs, train_labels)\n",
    "\n",
    "            #     loss = tool_l + phase_l\n",
    "            #     loss.backward()\n",
    "            #     optimizer.step()\n",
    "\n",
    "            soft_max_out = soft_max_out + train_outputs.cpu().detach().tolist()\n",
    "            predicted_labels = predicted_labels + train_preds.cpu().tolist()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total += train_labels.size(0)\n",
    "            train_correct += train_preds.eq(train_labels).sum().item()\n",
    "\n",
    "            \n",
    "        hmm_df = pd.DataFrame(\n",
    "            {'img_path': img_path,\n",
    "                'true_labels': true_labels,\n",
    "                'cnn_output': soft_max_out,\n",
    "                'predicted_labels': predicted_labels\n",
    "                })\n",
    "\n",
    "        \n",
    "        print(len(trainloader), 'Training Loss Phase: %.5f | Training Acc Phase: %.5f%% (%d/%d)'\n",
    "        % (train_loss / len(trainloader), 100. * train_correct / total, train_correct, total))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0.0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for j, (val_inputs,val_img_id, val_labels,val_tools,val_tool_targets) in enumerate(validationloader):\n",
    "                if device == 'cuda':\n",
    "                    val_inputs, val_labels, val_tools = val_inputs.to(device), val_labels.to(device), val_tools.to(device)\n",
    "\n",
    "                if model_num != 5:\n",
    "                    val_outputs = model.forward(val_inputs)\n",
    "                else:\n",
    "                    val_outputs, _ = model.forward(val_inputs)\n",
    "    \n",
    "                _, val_preds = torch.max(val_outputs.data, 1)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                # else:\n",
    "                #     tool_outputs, val_outputs = model.forward(val_inputs)\n",
    "                #     tool_outputs = tool_outputs.data\n",
    "\n",
    "\n",
    "                #     val_tool_outputs = (tool_outputs > 0.5).float()\n",
    "\n",
    "                #     tool_l = criterion_tool(val_tool_outputs, val_tools.float())\n",
    "        \n",
    "                #     _, val_preds = torch.max(val_outputs.data, 1)\n",
    "                #     phase_l = criterion(val_outputs, val_labels)\n",
    "                #     loss = tool_l + phase_l\n",
    "\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += val_preds.eq(val_labels).sum().item()\n",
    "\n",
    "        \n",
    "            print(len(validationloader), 'Validation Loss Phase: %.5f | Validation Acc Phase: %.5f%% (%d/%d)'\n",
    "                % (val_loss / len(validationloader), 100. * val_correct / val_total, val_correct, val_total))\n",
    "    \n",
    "            scheduler.step(val_loss)\n",
    "            #Get average for early stopping check and call the early stopping class to perform calc\n",
    "            val_loss_avg = val_loss/len(validationloader)\n",
    "            early_stopping(val_loss_avg, model, model_name, hmm_df)\n",
    "\n",
    "            #If limit reached stop execution\n",
    "            if early_stopping.stop:\n",
    "                break\n",
    "              \n",
    "    end_timer_and_print()\n",
    "    \n",
    "  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\cuda\\memory.py:278: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [06:17,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.75155 | Training Acc Phase: 37.47553% (32356/86339)\n",
      "84 Validation Loss Phase: 1.71584 | Validation Acc Phase: 35.89648% (7698/21445)\n",
      "Training Epoch: 2 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:53,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.65905 | Training Acc Phase: 39.66805% (34249/86339)\n",
      "84 Validation Loss Phase: 1.59922 | Validation Acc Phase: 35.89648% (7698/21445)\n",
      "Validation loss improved from 1.72 to 1.60  Saving Model\n",
      "Training Epoch: 3 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:54,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.49853 | Training Acc Phase: 41.45519% (35792/86339)\n",
      "84 Validation Loss Phase: 1.42711 | Validation Acc Phase: 39.01142% (8366/21445)\n",
      "Validation loss improved from 1.60 to 1.43  Saving Model\n",
      "Training Epoch: 4 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:47,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.29728 | Training Acc Phase: 43.41839% (37487/86339)\n",
      "84 Validation Loss Phase: 1.23114 | Validation Acc Phase: 38.44719% (8245/21445)\n",
      "Validation loss improved from 1.43 to 1.23  Saving Model\n",
      "Training Epoch: 5 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:44,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.14694 | Training Acc Phase: 48.13584% (41560/86339)\n",
      "84 Validation Loss Phase: 1.15234 | Validation Acc Phase: 43.65586% (9362/21445)\n",
      "Validation loss improved from 1.23 to 1.15  Saving Model\n",
      "Training Epoch: 6 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:39,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.05212 | Training Acc Phase: 52.31008% (45164/86339)\n",
      "84 Validation Loss Phase: 1.10079 | Validation Acc Phase: 46.36512% (9943/21445)\n",
      "Validation loss improved from 1.15 to 1.10  Saving Model\n",
      "Training Epoch: 7 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:43,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.99457 | Training Acc Phase: 55.07940% (47555/86339)\n",
      "84 Validation Loss Phase: 1.03924 | Validation Acc Phase: 49.37748% (10589/21445)\n",
      "Validation loss improved from 1.10 to 1.04  Saving Model\n",
      "Training Epoch: 8 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:42,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.89011 | Training Acc Phase: 58.34212% (50372/86339)\n",
      "84 Validation Loss Phase: 0.99609 | Validation Acc Phase: 50.23082% (10772/21445)\n",
      "Validation loss improved from 1.04 to 1.00  Saving Model\n",
      "Training Epoch: 9 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:43,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.77878 | Training Acc Phase: 62.34610% (53829/86339)\n",
      "84 Validation Loss Phase: 0.96011 | Validation Acc Phase: 52.38051% (11233/21445)\n",
      "Validation loss improved from 1.00 to 0.96  Saving Model\n",
      "Training Epoch: 10 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:44,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.68376 | Training Acc Phase: 66.84349% (57712/86339)\n",
      "84 Validation Loss Phase: 0.91696 | Validation Acc Phase: 62.91443% (13492/21445)\n",
      "Validation loss improved from 0.96 to 0.92  Saving Model\n",
      "Training Epoch: 11 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:38,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.59629 | Training Acc Phase: 72.48752% (62585/86339)\n",
      "84 Validation Loss Phase: 1.02177 | Validation Acc Phase: 66.08533% (14172/21445)\n",
      "Early Stopping count is at: 1 maximum is: 20\n",
      "Training Epoch: 12 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:48,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.48627 | Training Acc Phase: 79.01644% (68222/86339)\n",
      "84 Validation Loss Phase: 1.20148 | Validation Acc Phase: 57.16484% (12259/21445)\n",
      "Early Stopping count is at: 2 maximum is: 20\n",
      "Training Epoch: 13 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:34,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.38795 | Training Acc Phase: 83.93889% (72472/86339)\n",
      "84 Validation Loss Phase: 1.26519 | Validation Acc Phase: 59.19795% (12695/21445)\n",
      "Early Stopping count is at: 3 maximum is: 20\n",
      "Training Epoch: 14 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:24,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.30330 | Training Acc Phase: 87.35102% (75418/86339)\n",
      "84 Validation Loss Phase: 1.03004 | Validation Acc Phase: 63.84705% (13692/21445)\n",
      "Early Stopping count is at: 4 maximum is: 20\n",
      "Training Epoch: 15 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:30,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.26715 | Training Acc Phase: 88.94242% (76792/86339)\n",
      "84 Validation Loss Phase: 1.54067 | Validation Acc Phase: 50.99557% (10936/21445)\n",
      "Early Stopping count is at: 5 maximum is: 20\n",
      "Training Epoch: 16 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:28,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.22732 | Training Acc Phase: 89.34317% (77138/86339)\n",
      "84 Validation Loss Phase: 1.08422 | Validation Acc Phase: 65.59571% (14067/21445)\n",
      "Early Stopping count is at: 6 maximum is: 20\n",
      "Training Epoch: 17 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:17,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.15120 | Training Acc Phase: 93.22786% (80492/86339)\n",
      "84 Validation Loss Phase: 0.90441 | Validation Acc Phase: 76.69387% (16447/21445)\n",
      "Validation loss improved from 0.92 to 0.90  Saving Model\n",
      "Training Epoch: 18 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:30,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.31059 | Training Acc Phase: 87.63710% (75665/86339)\n",
      "84 Validation Loss Phase: 0.83330 | Validation Acc Phase: 74.08254% (15887/21445)\n",
      "Validation loss improved from 0.90 to 0.83  Saving Model\n",
      "Training Epoch: 19 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:34,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.21761 | Training Acc Phase: 91.09093% (78647/86339)\n",
      "84 Validation Loss Phase: 1.28317 | Validation Acc Phase: 59.73420% (12810/21445)\n",
      "Early Stopping count is at: 1 maximum is: 20\n",
      "Training Epoch: 20 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:36,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.14619 | Training Acc Phase: 94.02240% (81178/86339)\n",
      "84 Validation Loss Phase: 0.68464 | Validation Acc Phase: 77.94824% (16716/21445)\n",
      "Validation loss improved from 0.83 to 0.68  Saving Model\n",
      "Training Epoch: 21 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:35,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.09674 | Training Acc Phase: 95.70878% (82634/86339)\n",
      "84 Validation Loss Phase: 1.18775 | Validation Acc Phase: 69.71788% (14951/21445)\n",
      "Early Stopping count is at: 1 maximum is: 20\n",
      "Training Epoch: 22 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:35,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.17709 | Training Acc Phase: 92.98000% (80278/86339)\n",
      "84 Validation Loss Phase: 1.18004 | Validation Acc Phase: 68.58941% (14709/21445)\n",
      "Early Stopping count is at: 2 maximum is: 20\n",
      "Training Epoch: 23 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:29,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.10242 | Training Acc Phase: 95.97864% (82867/86339)\n",
      "84 Validation Loss Phase: 0.88101 | Validation Acc Phase: 75.17370% (16121/21445)\n",
      "Early Stopping count is at: 3 maximum is: 20\n",
      "Training Epoch: 24 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:27,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.05562 | Training Acc Phase: 97.74957% (84396/86339)\n",
      "84 Validation Loss Phase: 0.98679 | Validation Acc Phase: 74.87993% (16058/21445)\n",
      "Early Stopping count is at: 4 maximum is: 20\n",
      "Training Epoch: 25 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:27,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.04405 | Training Acc Phase: 98.12483% (84720/86339)\n",
      "84 Validation Loss Phase: 1.14157 | Validation Acc Phase: 71.19142% (15267/21445)\n",
      "Early Stopping count is at: 5 maximum is: 20\n",
      "Training Epoch: 26 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:28,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.03302 | Training Acc Phase: 98.67036% (85191/86339)\n",
      "84 Validation Loss Phase: 1.02560 | Validation Acc Phase: 74.59548% (15997/21445)\n",
      "Early Stopping count is at: 6 maximum is: 20\n",
      "Training Epoch: 27 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [02:10,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel_train_validate_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 147\u001b[0m, in \u001b[0;36mmodel_train_validate_cnn\u001b[1;34m(train_df, val_df, epochs, model_num, optimizer_num, batch_size, learning_rate, weight_decay, momentum, patience, patience_delta)\u001b[0m\n\u001b[0;32m    118\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m       \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m#     loss.backward()\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m#     optimizer.step()\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m soft_max_out \u001b[38;5;241m=\u001b[39m soft_max_out \u001b[38;5;241m+\u001b[39m \u001b[43mtrain_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    148\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m predicted_labels \u001b[38;5;241m+\u001b[39m train_preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    150\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = None\n",
    "model_train_validate_cnn(train_df, val_df,batch_size=256, model_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_validate_tool_detect(train_df, val_df, method=0, device='cuda',epochs=100, optimizer_num = 0,batch_size = 128, learning_rate = 1e-3, weight_decay=5e-4,momentum=0.9,patience=20, patience_delta = 0.0005):\n",
    "    '''\n",
    "    Model nums:\n",
    "    CNN tool detection\n",
    "    Model 0: ResNet50\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #Get train label weights and convert to torch tensor\n",
    "    tool_weights=class_weight.compute_class_weight('balanced',classes=np.unique(train_df['tool_target']),y=train_df['tool_target'].to_numpy())\n",
    "    tool_weights=torch.tensor(tool_weights,dtype=torch.float)\n",
    "    print(f'Weighted classes: {tool_weights}')\n",
    "\n",
    "    #Create datasets for phase segmentation\n",
    "    trainset = Cholec80Dataset(train_df,  get_transform_train())\n",
    "    validationset = Cholec80Dataset(val_df,  get_transform_valid())\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    validationloader = torch.utils.data.DataLoader(\n",
    "        validationset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    #Initialise tool detection loss\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    criterion_tool = nn.CrossEntropyLoss(weight=tool_weights)\n",
    "    if device == 'cuda':\n",
    "        criterion.to(device)\n",
    "        criterion_tool.to(device)\n",
    "        \n",
    "    #Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience, patience_delta)\n",
    "\n",
    "\n",
    "    #Model selection\n",
    "    model = ResNet50_ST()\n",
    "    model_name = 'ResNet50_od'\n",
    "\n",
    "    # elif model_num == 2:\n",
    "    #     model = DeiT_OD_phase()\n",
    "    #     model_name = 'DeiT3_ord_od'\n",
    "    #     model.freeze()\n",
    "\n",
    "    #Final calculated learning rate\n",
    "    learning_rate_final = learning_rate * batch_size / 512\n",
    "\n",
    "    #Select optimizer for job\n",
    "    optimizer = None\n",
    "    if optimizer_num == 0:\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate_final, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_num == 1:\n",
    "        # optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate_final, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
    "\n",
    "\n",
    "    if device == 'cuda':\n",
    "        model.to(device)\n",
    "\n",
    "    start_timer()\n",
    "    for epoch in range(0,epochs):\n",
    "        print(f'Training Epoch: {epoch+1} of {epochs}..')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0.0\n",
    "        batch_gt = []\n",
    "        batch_pred = []\n",
    "\n",
    "        # Outputs for HMM\n",
    "        img_path = []\n",
    "        true_labels = []\n",
    "        softmax_out = []\n",
    "        predicted_labels = []\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        for i, (train_inputs,train_img_id, train_labels,train_tools,train_tool_targets) in tqdm(enumerate(trainloader, 0)):\n",
    "            img_path = img_path + list(train_img_id)\n",
    "            true_labels = true_labels + train_tool_targets.tolist()\n",
    "\n",
    "            if device == 'cuda':\n",
    "                train_inputs, train_labels, train_tools,train_tool_targets = train_inputs.to(device), train_labels.to(device), train_tools.to(device), train_tool_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_outputs, sigmoid_outputs  = model.forward(train_inputs)\n",
    "            if method ==0:\n",
    "                train_preds = (sigmoid_outputs > 0.5).float()\n",
    "            else:\n",
    "                _, train_preds = torch.max(train_outputs.data, 1)\n",
    "\n",
    "            if method == 0:\n",
    "                loss = criterion(train_outputs, train_tools)\n",
    "            else:\n",
    "                loss = criterion_tool(train_outputs, train_tool_targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            softmax_out = softmax_out + train_outputs.cpu().detach().tolist()\n",
    "            predicted_labels = predicted_labels + train_preds.cpu().tolist()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if method == 0:\n",
    "                batch_gt.append(np.concatenate(train_tools.cpu().tolist()))\n",
    "                batch_pred.append(np.concatenate(train_preds.cpu().tolist()))\n",
    "            else:\n",
    "                total += train_labels.size(0)\n",
    "                train_correct += train_preds.eq(train_tool_targets).sum().item()\n",
    "                \n",
    "        if method == 0:\n",
    "            all_batch_gt = np.concatenate(batch_gt)\n",
    "            all_batch_pred = np.concatenate(batch_pred)\n",
    "\n",
    "        hmm_df = pd.DataFrame(\n",
    "            {'img_path': img_path,\n",
    "                'true_labels': true_labels,\n",
    "                'cnn_output': softmax_out,\n",
    "                'predicted_labels': predicted_labels\n",
    "                })\n",
    "        if method == 0:\n",
    "            print(len(trainloader), 'Training Loss Phase: %.5f | Training Acc Phase: %.5f%%'\n",
    "            % (train_loss / len(trainloader),100. * accuracy_score(all_batch_gt,all_batch_pred)))\n",
    "        else:\n",
    "            print(len(trainloader), 'Training Loss Phase: %.5f | Training Acc Phase: %.5f%% (%d/%d)'\n",
    "            % (train_loss / len(trainloader), 100. * train_correct / total, train_correct, total))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0.0\n",
    "        val_batch_gt = []\n",
    "        val_batch_pred = []\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for j, (val_inputs,val_img_id, val_labels,val_tools,val_tool_targets) in enumerate(validationloader):\n",
    "                if device == 'cuda':\n",
    "                    val_inputs, val_labels, val_tools, val_tool_targets = val_inputs.to(device), val_labels.to(device), val_tools.to(device), val_tool_targets.to(device)\n",
    "\n",
    "                val_outputs, val_sigmoid_outputs = model.forward(val_inputs)\n",
    "                if method == 0:\n",
    "                    val_preds = (val_sigmoid_outputs > 0.5).float()\n",
    "                else:\n",
    "                    _, val_preds = torch.max(val_outputs.data, 1)\n",
    "\n",
    "                if method == 0:\n",
    "                    loss = criterion(val_outputs, val_tools)\n",
    "                else:\n",
    "                    loss = criterion_tool(val_outputs, val_tool_targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                if method == 0:\n",
    "                    val_batch_gt.append(np.concatenate(val_tools.cpu().tolist()))\n",
    "                    val_batch_pred.append(np.concatenate(val_preds.cpu().tolist()))\n",
    "                else:\n",
    "                    val_total += val_labels.size(0)\n",
    "                    val_correct += val_preds.eq(val_tool_targets).sum().item()\n",
    "\n",
    "            if method == 0:\n",
    "                all_batch_gt = np.concatenate(val_batch_gt)\n",
    "                all_batch_pred = np.concatenate(val_batch_pred)\n",
    "\n",
    "            if method == 0:\n",
    "                print(len(validationloader), 'Validation Loss Phase: %.5f | Validation Acc Phase: %.5f%%'\n",
    "                % (val_loss / len(validationloader), 100. * accuracy_score(all_batch_gt,all_batch_pred)))\n",
    "            else:\n",
    "                print(len(validationloader), 'Validation Loss Phase: %.5f | Validation Acc Phase: %.5f%% (%d/%d)'\n",
    "                % (val_loss / len(validationloader), 100. * val_correct / val_total, val_correct, val_total))\n",
    "    \n",
    "            scheduler.step(val_loss)\n",
    "            #Get average for early stopping check and call the early stopping class to perform calc\n",
    "            val_loss_avg = val_loss/len(validationloader)\n",
    "            early_stopping(val_loss_avg, model, model_name, hmm_df)\n",
    "\n",
    "            #If limit reached stop execution\n",
    "            if early_stopping.stop:\n",
    "                break\n",
    "\n",
    "    end_timer_and_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted classes: tensor([3.5000e-01, 2.6292e-01, 9.2914e-02, 2.0479e-01, 1.4761e+00, 2.3115e+00,\n",
      "        2.8420e+00, 1.9868e+00, 1.4197e+00, 2.9995e+00, 3.1497e+00, 4.3800e+00,\n",
      "        8.7081e-01, 5.6997e+00, 6.1303e+00, 1.3465e+01, 2.4280e+01, 9.0960e+00,\n",
      "        1.0560e+01, 1.5813e+01, 5.7852e+00, 2.5696e+02, 7.7088e+02, 3.0835e+03,\n",
      "        1.1013e+02, 5.4097e+01, 2.2673e+01, 4.0573e+01])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\cuda\\memory.py:278: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:54,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 146.00623 | Training Acc Phase: 96.94510%\n",
      "84 Validation Loss Phase: 264.77233 | Validation Acc Phase: 95.32025%\n",
      "Training Epoch: 2 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:46,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 68.75395 | Training Acc Phase: 98.63826%\n",
      "84 Validation Loss Phase: 256.70888 | Validation Acc Phase: 95.66732%\n",
      "Validation loss improved from 264.77 to 256.71  Saving Model\n",
      "Training Epoch: 3 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 49.50459 | Training Acc Phase: 99.03569%\n",
      "84 Validation Loss Phase: 187.66276 | Validation Acc Phase: 97.03827%\n",
      "Validation loss improved from 256.71 to 187.66  Saving Model\n",
      "Training Epoch: 4 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 40.50297 | Training Acc Phase: 99.19503%\n",
      "84 Validation Loss Phase: 284.64410 | Validation Acc Phase: 96.28818%\n",
      "Early Stopping count is at: 1 maximum is: 20\n",
      "Training Epoch: 5 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:50,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 32.84200 | Training Acc Phase: 99.33865%\n",
      "84 Validation Loss Phase: 233.43043 | Validation Acc Phase: 96.80245%\n",
      "Early Stopping count is at: 2 maximum is: 20\n",
      "Training Epoch: 6 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:48,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 27.09414 | Training Acc Phase: 99.45828%\n",
      "84 Validation Loss Phase: 225.79697 | Validation Acc Phase: 96.90038%\n",
      "Early Stopping count is at: 3 maximum is: 20\n",
      "Training Epoch: 7 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 22.65830 | Training Acc Phase: 99.54283%\n",
      "84 Validation Loss Phase: 375.91159 | Validation Acc Phase: 95.99241%\n",
      "Early Stopping count is at: 4 maximum is: 20\n",
      "Training Epoch: 8 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 20.33195 | Training Acc Phase: 99.59462%\n",
      "84 Validation Loss Phase: 288.94266 | Validation Acc Phase: 96.39543%\n",
      "Early Stopping count is at: 5 maximum is: 20\n",
      "Training Epoch: 9 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 18.12771 | Training Acc Phase: 99.63367%\n",
      "84 Validation Loss Phase: 274.01960 | Validation Acc Phase: 96.91969%\n",
      "Early Stopping count is at: 6 maximum is: 20\n",
      "Training Epoch: 10 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 15.74508 | Training Acc Phase: 99.68579%\n",
      "84 Validation Loss Phase: 293.96703 | Validation Acc Phase: 96.73517%\n",
      "Early Stopping count is at: 7 maximum is: 20\n",
      "Training Epoch: 11 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:47,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 13.56817 | Training Acc Phase: 99.72500%\n",
      "84 Validation Loss Phase: 359.32754 | Validation Acc Phase: 96.35280%\n",
      "Early Stopping count is at: 8 maximum is: 20\n",
      "Training Epoch: 12 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 11.41628 | Training Acc Phase: 99.77216%\n",
      "84 Validation Loss Phase: 311.14849 | Validation Acc Phase: 96.84708%\n",
      "Early Stopping count is at: 9 maximum is: 20\n",
      "Training Epoch: 13 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:45,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 10.94884 | Training Acc Phase: 99.78705%\n",
      "84 Validation Loss Phase: 311.13645 | Validation Acc Phase: 96.76448%\n",
      "Early Stopping count is at: 10 maximum is: 20\n",
      "Training Epoch: 14 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:44,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 9.75946 | Training Acc Phase: 99.80790%\n",
      "84 Validation Loss Phase: 404.63814 | Validation Acc Phase: 96.13963%\n",
      "Epoch 00014: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Early Stopping count is at: 11 maximum is: 20\n",
      "Training Epoch: 15 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:44,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 4.71298 | Training Acc Phase: 99.91611%\n",
      "84 Validation Loss Phase: 333.35540 | Validation Acc Phase: 97.05559%\n",
      "Early Stopping count is at: 12 maximum is: 20\n",
      "Training Epoch: 16 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:45,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 2.72536 | Training Acc Phase: 99.95648%\n",
      "84 Validation Loss Phase: 325.29377 | Validation Acc Phase: 97.16018%\n",
      "Early Stopping count is at: 13 maximum is: 20\n",
      "Training Epoch: 17 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:44,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.98381 | Training Acc Phase: 99.97187%\n",
      "84 Validation Loss Phase: 354.66527 | Validation Acc Phase: 97.09889%\n",
      "Early Stopping count is at: 14 maximum is: 20\n",
      "Training Epoch: 18 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:43,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.57729 | Training Acc Phase: 99.98048%\n",
      "84 Validation Loss Phase: 342.04924 | Validation Acc Phase: 97.21280%\n",
      "Early Stopping count is at: 15 maximum is: 20\n",
      "Training Epoch: 19 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:46,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.32506 | Training Acc Phase: 99.98163%\n",
      "84 Validation Loss Phase: 369.90087 | Validation Acc Phase: 97.13353%\n",
      "Early Stopping count is at: 16 maximum is: 20\n",
      "Training Epoch: 20 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:48,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.17439 | Training Acc Phase: 99.98527%\n",
      "84 Validation Loss Phase: 378.90439 | Validation Acc Phase: 97.13153%\n",
      "Early Stopping count is at: 17 maximum is: 20\n",
      "Training Epoch: 21 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [05:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 1.06828 | Training Acc Phase: 99.98776%\n",
      "84 Validation Loss Phase: 367.36948 | Validation Acc Phase: 97.21613%\n",
      "Early Stopping count is at: 18 maximum is: 20\n",
      "Training Epoch: 22 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:45,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.94571 | Training Acc Phase: 99.98742%\n",
      "84 Validation Loss Phase: 388.74334 | Validation Acc Phase: 97.12420%\n",
      "Early Stopping count is at: 19 maximum is: 20\n",
      "Training Epoch: 23 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:47,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.76478 | Training Acc Phase: 99.99206%\n",
      "84 Validation Loss Phase: 401.01594 | Validation Acc Phase: 97.08690%\n",
      "Early Stopping count is at: 20 maximum is: 20\n",
      "Training Epoch: 24 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:48,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.79704 | Training Acc Phase: 99.99123%\n",
      "84 Validation Loss Phase: 387.36524 | Validation Acc Phase: 97.20947%\n",
      "Early Stopping count is at: 21 maximum is: 20\n",
      "Training Epoch: 25 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:48,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.65518 | Training Acc Phase: 99.99255%\n",
      "84 Validation Loss Phase: 401.36209 | Validation Acc Phase: 97.14152%\n",
      "Epoch 00025: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Early Stopping count is at: 22 maximum is: 20\n",
      "Training Epoch: 26 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:48,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.57756 | Training Acc Phase: 99.99421%\n",
      "84 Validation Loss Phase: 394.58029 | Validation Acc Phase: 97.15085%\n",
      "Early Stopping count is at: 23 maximum is: 20\n",
      "Training Epoch: 27 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:48,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.58262 | Training Acc Phase: 99.99404%\n",
      "84 Validation Loss Phase: 411.39350 | Validation Acc Phase: 97.10555%\n",
      "Early Stopping count is at: 24 maximum is: 20\n",
      "Training Epoch: 28 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.55745 | Training Acc Phase: 99.99437%\n",
      "84 Validation Loss Phase: 404.60815 | Validation Acc Phase: 97.12953%\n",
      "Early Stopping count is at: 25 maximum is: 20\n",
      "Training Epoch: 29 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [04:49,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 Training Loss Phase: 0.55790 | Training Acc Phase: 99.99487%\n",
      "84 Validation Loss Phase: 401.40203 | Validation Acc Phase: 97.18882%\n",
      "Early Stopping count is at: 26 maximum is: 20\n",
      "Training Epoch: 30 of 100..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:33,  1.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_validate_tool_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m, in \u001b[0;36mtrain_validate_tool_detect\u001b[1;34m(train_df, val_df, method, device, epochs, optimizer_num, batch_size, learning_rate, weight_decay, momentum, patience, patience_delta)\u001b[0m\n\u001b[0;32m    102\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion_tool(train_outputs, train_tool_targets)\n\u001b[0;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 105\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m softmax_out \u001b[38;5;241m=\u001b[39m softmax_out \u001b[38;5;241m+\u001b[39m train_outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    108\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m predicted_labels \u001b[38;5;241m+\u001b[39m train_preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py:146\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m             momentum_buffer_list\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmomentum_buffer\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 146\u001b[0m sgd(params_with_grad,\n\u001b[0;32m    147\u001b[0m     d_p_list,\n\u001b[0;32m    148\u001b[0m     momentum_buffer_list,\n\u001b[0;32m    149\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    150\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m    156\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    158\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py:197\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 197\u001b[0m func(params,\n\u001b[0;32m    198\u001b[0m      d_p_list,\n\u001b[0;32m    199\u001b[0m      momentum_buffer_list,\n\u001b[0;32m    200\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    201\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[0;32m    202\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    203\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[0;32m    204\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[0;32m    205\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m    206\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[1;32mc:\\Users\\vkula\\anaconda3\\lib\\site-packages\\torch\\optim\\sgd.py:233\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    231\u001b[0m     momentum_buffer_list[i] \u001b[39m=\u001b[39m buf\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     buf\u001b[39m.\u001b[39;49mmul_(momentum)\u001b[39m.\u001b[39madd_(d_p, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dampening)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m nesterov:\n\u001b[0;32m    236\u001b[0m     d_p \u001b[39m=\u001b[39m d_p\u001b[39m.\u001b[39madd(buf, alpha\u001b[39m=\u001b[39mmomentum)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = None\n",
    "train_validate_tool_detect(train_df, val_df, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_loss_history = []\n",
    "# running_corrects_history = []\n",
    "# val_running_loss_history = []\n",
    "# val_running_corrects_history = []\n",
    "\n",
    "# start_timer()\n",
    "# # training epochs\n",
    "# for epoch in range(0, EPOCHS):\n",
    "\n",
    "#     correct = 0.0\n",
    "#     total = 0.0\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     # Outputs for HMM\n",
    "#     img_path = []\n",
    "#     true_labels = []\n",
    "#     soft_max_out = []\n",
    "#     predicted_labels = []\n",
    "\n",
    "#     for i, data in tqdm(enumerate(trainloader, 0)):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs,img_id, labels,tools  = data\n",
    "#         img_path = img_path + list(img_id)\n",
    "\n",
    "\n",
    "#         true_labels = true_labels + labels.tolist()\n",
    "        \n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         inputs_tools = tools.to(device)\n",
    "\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         if USE_TOOL:\n",
    "#             outputs = model(inputs, inputs_tools)\n",
    "#         else:\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#         soft_max_out = soft_max_out + outputs.cpu().detach().tolist()\n",
    "\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         predicted_labels = predicted_labels + predicted.cpu().tolist()\n",
    "\n",
    "#         total += labels.size(0)\n",
    "#         correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#     hmm_df = pd.DataFrame(\n",
    "#         {'img_path': img_path,\n",
    "#             'true_labels': true_labels,\n",
    "#             'cnn_output': soft_max_out,\n",
    "#             'predicted_labels': predicted_labels\n",
    "#             })\n",
    "\n",
    "#     print(len(trainloader), 'Training Loss: %.5f | Training Acc: %.5f%% (%d/%d)'\n",
    "#             % (running_loss / len(trainloader), 100. * correct / total, correct, total))\n",
    "\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_correct = 0.0\n",
    "#     val_total = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for j, (val_inputs,val_img_id, val_labels,val_tools) in enumerate(validationloader):\n",
    "#             val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "#             val_inputs_tools = val_tools.to(device)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "#             if USE_TOOL:\n",
    "#                 val_outputs = model(val_inputs, val_inputs_tools)\n",
    "#             else:\n",
    "#                 val_outputs = model(val_inputs)\n",
    "\n",
    "#             validation_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "#             val_loss += validation_loss.item()\n",
    "#             _, val_predicted = val_outputs.max(1)\n",
    "#             val_total += val_labels.size(0)\n",
    "#             val_correct += val_predicted.eq(val_labels).sum().item()\n",
    "\n",
    "#         print(len(validationloader), 'Validation Loss: %.5f | Validation Acc: %.5f%% (%d/%d)'\n",
    "#                 % (val_loss / len(validationloader), 100. * val_correct / val_total, val_correct, val_total))\n",
    "#         scheduler.step(val_loss)\n",
    "        \n",
    "#         #Get average for early stopping check and call the early stopping class to perform calc\n",
    "#         val_loss_avg = val_loss/len(validationloader)\n",
    "#         early_stopping(val_loss_avg, model, MODEL_NAME)\n",
    "\n",
    "#         #If limit reached stop execution\n",
    "#         if early_stopping.stop:\n",
    "#             break\n",
    "\n",
    "#     epoch_loss = running_loss / len(trainloader)  # loss per epoch\n",
    "#     epoch_acc = correct / total  # accuracy per epoch\n",
    "#     running_loss_history.append(epoch_loss)  # appending for displaying\n",
    "#     running_corrects_history.append(epoch_acc)\n",
    "\n",
    "#     val_epoch_loss = val_loss / len(validationloader)\n",
    "#     val_epoch_acc = val_correct / val_total\n",
    "#     val_running_loss_history.append(val_epoch_loss)\n",
    "#     val_running_corrects_history.append(val_epoch_acc)\n",
    "\n",
    "\n",
    "# plt.style.use('ggplot')\n",
    "# plt.plot(running_loss_history, label='training loss')\n",
    "# plt.plot(val_running_loss_history, label='validation loss')\n",
    "# plt.title(\"Training validation loss curve\")\n",
    "# plt.legend()\n",
    "# print(\"Finished Training\")\n",
    "# end_timer_and_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9136203666cc20a7cbb4159ad8ff6a3bdf9f9662fc68256af5fa565dcf4d2f30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
